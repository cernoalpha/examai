import * as axios from 'axios';
import { Axios, CreateAxiosDefaults, AxiosRequestConfig } from 'axios';

/* eslint-disable @typescript-eslint/ban-types */
/* eslint-disable @typescript-eslint/member-delimiter-style */
/**
 * This file was automatically generated by openapi2ts.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source
 * openapi definition and regenerate this file.
 */
type APIResponseValidationError$1 = {
  message: string
  /**
   * * `APIResponseValidationError` - APIResponseValidationError
   */
  code: "APIResponseValidationError"
  [k: string]: unknown
}

type AuthenticationError$1 = {
  message: string
  /**
   * * `AuthenticationError` - AuthenticationError
   */
  code: "AuthenticationError"
  [k: string]: unknown
}

type CatchAllError$1 = {
  message: string
  /**
   * * `CatchAllError` - CatchAllError
   */
  code: "CatchAllError"
  [k: string]: unknown
}

type ChatCompletionInput = {
  /**
   * The ID of the project to use.
   */
  project_id: number
  /**
   * The ID of the session to use. It helps to track the chat history.
   */
  session_id?: string
  /**
   * Options for Retrieval Augmented Generation (RAG). Will override launched model settings
   */
  repositories?: {
    /**
     * The IDs of the repositories to use.
     */
    ids?: number[]
    limit?: number
    similarity_threshold?: number
    [k: string]: unknown
  }
  /**
   * A list of messages comprising the conversation so far.
   */
  messages: {
    /**
     * The role of the sender (e.g., 'user' or 'assistant').
     *
     * * `user` - user
     * * `assistant` - assistant
     */
    role: "user" | "assistant"
    /**
     * The content of the message.
     */
    content?: string
    /**
     * The ID of the template to use.
     */
    template_id?: string
    /**
     * The parameters (key: value) to use with the given template.
     */
    params?: {
      [k: string]: unknown
    }
    [k: string]: unknown
  }[]
  /**
   * ID of the model to use. See the model endpoint compatibility table for details.
   */
  model?: string
  /**
   * The system prompt to use.
   */
  system_prompt?: string
  /**
   * The maximum number of tokens to generate in the chat completion.
   */
  max_tokens?: number
  /**
   * If set, partial message deltas will be sent, like in ChatGPT.
   */
  stream?: boolean
  /**
   * What sampling temperature to use, between 0 and 2.
   */
  temperature?: number
  /**
   * The tools to use in the completion.
   */
  tools?: {
    /**
     * * `function` - function
     */
    type: "function"
    function: {
      name: string
      description?: string
      parameters: {
        type: string
        properties: {
          [k: string]: {
            type: string
            description: string
            [k: string]: unknown
          }
        }
        required: string[]
        [k: string]: unknown
      }
      [k: string]: unknown
    }
    [k: string]: unknown
  }[]
  [k: string]: unknown
}

type ChatCompletionResponse = {
  /**
   * A list of chat completion choices. Can be more than one if n is greater than 1.
   */
  choices: {
    /**
     * The index of the choice in the list of choices.
     */
    index: number
    /**
     * The messages in the chat completion.
     */
    message: {
      /**
       * The role of the sender (e.g., 'user' or 'assistant').
       *
       * * `user` - user
       * * `assistant` - assistant
       */
      role: "user" | "assistant"
      /**
       * The content of the message.
       */
      content?: string
      /**
       * The ID of the template to use.
       */
      template_id?: string
      /**
       * The parameters (key: value) to use with the given template.
       */
      params?: {
        [k: string]: unknown
      }
      [k: string]: unknown
    }
    /**
     * The reason the chat completion finished, e.g., 'stop' or 'length'.
     */
    finish_reason: string
    /**
     * The tool calls made.
     */
    tool_calls?: {
      /**
       * The ID of the tool call.
       */
      id: string
      /**
       * The function called.
       */
      function: {
        /**
         * The name of the function to be called.
         */
        name: string
        /**
         * The arguments passed to the function.
         */
        arguments?: {
          [k: string]: unknown
        }
        [k: string]: unknown
      }
      /**
       * The type of tool call.
       */
      type: string
      [k: string]: unknown
    }[]
    [k: string]: unknown
  }[]
  /**
   * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
   */
  created: number
  /**
   * The model to generate the completion.
   */
  model: string
  /**
   * The name of the provider that generated the completion.
   */
  provider_name: string
  /**
   * The ID of the provider that generated the completion.
   */
  provider_id: string
  /**
   * Chunks used to improve the completion
   */
  document_chunks?: {
    repository_id?: number
    document_id?: number
    chunk_id?: number
    document_name?: string
    similarity_score?: number
    content?: string
    [k: string]: unknown
  }[]
  /**
   * The usage statistics for the completion.
   */
  usage: {
    prompt_tokens: number
    completion_tokens?: number
    total_tokens: number
    [k: string]: unknown
  }
  /**
   * The trace ID of the completion.
   */
  trace_id: string
  [k: string]: unknown
}

type ConflictError$1 = {
  message: string
  /**
   * * `ConflictError` - ConflictError
   */
  code: "ConflictError"
  [k: string]: unknown
}

type EmbeddingsInput = {
  /**
   * The ID of the project to use.
   */
  project_id: number
  /**
   * The model to generate the embeddings.
   */
  model: string
  encoding_format?: ("float" | "base64") & string
  /**
   * Embedding Input
   */
  input: string | string[] | number[] | number[][]
  [k: string]: unknown
}

type EmbeddingsResponse = {
  /**
   * The embeddings for the input.
   */
  data: {
    /**
     * The index of the token in the input.
     */
    index: number
    /**
     * The embedding for the input.
     */
    embedding: number[]
    [k: string]: unknown
  }[]
  /**
   * The model to generate the embeddings.
   */
  model: string
  /**
   * The usage statistics for the completion.
   */
  usage: {
    prompt_tokens: number
    completion_tokens?: number
    total_tokens: number
    [k: string]: unknown
  }
  /**
   * The name of the provider that generated the completion.
   */
  provider_name: string
  /**
   * The ID of the provider that generated the completion.
   */
  provider_id: string
  [k: string]: unknown
}

type InternalServerError$1 =
  | {
      message: string
      /**
       * * `ProviderInternalServerError` - ProviderInternalServerError
       */
      code: "ProviderInternalServerError"
      [k: string]: unknown
    }
  | {
      message: string
      /**
       * * `APIResponseValidationError` - APIResponseValidationError
       */
      code: "APIResponseValidationError"
      [k: string]: unknown
    }
  | {
      message: string
      /**
       * * `ProviderAPIStatusError` - ProviderAPIStatusError
       */
      code: "ProviderAPIStatusError"
      [k: string]: unknown
    }
  | {
      message: string
      /**
       * * `ProviderAPITimeoutError` - ProviderAPITimeoutError
       */
      code: "ProviderAPITimeoutError"
      [k: string]: unknown
    }
  | {
      message: string
      /**
       * * `ProviderAPIConnectionError` - ProviderAPIConnectionError
       */
      code: "ProviderAPIConnectionError"
      [k: string]: unknown
    }
  | {
      message: string
      /**
       * * `CatchAllError` - CatchAllError
       */
      code: "CatchAllError"
      [k: string]: unknown
    }

type ModelNotFoundError$1 = {
  message: string
  /**
   * * `ModelNotFoundError` - ModelNotFoundError
   */
  code: "ModelNotFoundError"
  [k: string]: unknown
}

type NotFoundError$1 =
  | {
      message: string
      /**
       * * `ProviderNotFoundError` - ProviderNotFoundError
       */
      code: "ProviderNotFoundError"
      [k: string]: unknown
    }
  | {
      message: string
      /**
       * * `ModelNotFoundError` - ModelNotFoundError
       */
      code: "ModelNotFoundError"
      [k: string]: unknown
    }

type PermissionDeniedError$1 = {
  message: string
  /**
   * * `PermissionDeniedError` - PermissionDeniedError
   */
  code: "PermissionDeniedError"
  [k: string]: unknown
}

type ProviderAPIConnectionError$1 = {
  message: string
  /**
   * * `ProviderAPIConnectionError` - ProviderAPIConnectionError
   */
  code: "ProviderAPIConnectionError"
  [k: string]: unknown
}

type ProviderAPIStatusError$1 = {
  message: string
  /**
   * * `ProviderAPIStatusError` - ProviderAPIStatusError
   */
  code: "ProviderAPIStatusError"
  [k: string]: unknown
}

type ProviderAPITimeoutError$1 = {
  message: string
  /**
   * * `ProviderAPITimeoutError` - ProviderAPITimeoutError
   */
  code: "ProviderAPITimeoutError"
  [k: string]: unknown
}

type ProviderInternalServerError$1 = {
  message: string
  /**
   * * `ProviderInternalServerError` - ProviderInternalServerError
   */
  code: "ProviderInternalServerError"
  [k: string]: unknown
}

type ValidationError$1 = {
  /**
   * A description of the validation error.
   */
  message: string
  /**
   * Detailed information about the validation errors.
   */
  details: {
    [k: string]: {
      /**
       * Error messages for the field.
       */
      error_messages: {
        [k: string]: unknown
      }[]
      [k: string]: unknown
    }
  }
  /**
   * * `ValidationError` - ValidationError
   */
  code: "ValidationError"
  [k: string]: unknown
}

type PremBaseConfig = {
  baseUrl?: string,
  apiKey: string
}

// CHAT COMPLETION - CREATE

type CreateChatCompletionRequest = ChatCompletionInput
type CreateChatCompletionResponse = ChatCompletionResponse

type ChatCompletionStreamingCompletionData = {
  id: string,
  model: string,
  object: string,
  created: number,
  choices: {
    finish_reason: string | null,
    delta: {
      content: string,
      role: string | null
    }
  }[]
}

type DocumentChunk = {
  repository_id?: number;
  document_id?: number;
  chunk_id?: string;
  document_name?: string;
  /** Format: double */
  similarity_score?: number;
  content?: string;
}

type ChatCompletionStreamingMessage = {
  event: "completion",
  data: ChatCompletionStreamingCompletionData
} | {
  event: "done",
  data: {
    traceId: string,
    documentChunks: DocumentChunk[]
  }
}

// EMBEDDING - CREATE

type CreateEmbeddingRequest = EmbeddingsInput
type CreateEmbeddingResponse = EmbeddingsResponse

// DATAPOINT - CREATE
type CreateDataPointRequest = undefined
type CreateDataPointResponse = undefined

// DATAPOINT - RETRIEVE
type RetrieveDataPointResponse = undefined

// DATAPOINT - LIST
type ListDataPointResponse = undefined[]

// DATAPOINT - UPDATE
type UpdateDataPointResponse = undefined
type UpdateDataPointRequest = {
  input?: string
  output?: string
  positive?: boolean
  trace?: string
}


// ERRORS

type ProviderInternalServerError = ProviderInternalServerError$1
type APIResponseValidationError = APIResponseValidationError$1
type ProviderAPIStatusError = ProviderAPIStatusError$1
type ProviderAPITimeoutError = ProviderAPITimeoutError$1
type ProviderAPIConnectionError = ProviderAPIConnectionError$1
type CatchAllError = CatchAllError$1
type AuthenticationError = AuthenticationError$1
type ConflictError = ConflictError$1
type InternalServerError = InternalServerError$1
type ModelNotFoundError = ModelNotFoundError$1
type NotFoundError = NotFoundError$1
type PermissionDeniedError = PermissionDeniedError$1
type ValidationError = ValidationError$1

declare class APIModule {
    protected client: Prem;
    constructor(client: Prem);
}

declare class export_default$6 extends APIModule {
    create: <T extends boolean>(args: Omit<ChatCompletionInput, "stream"> & {
        stream: T;
    }, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<T extends true ? AsyncIterable<ChatCompletionStreamingCompletionData> & {
        trace_id: string | null;
        document_chunks: DocumentChunk[] | null;
    } : ChatCompletionResponse>;
}

declare class export_default$5 extends APIModule {
    create: (params: {
        project_id: number;
        model: string;
        encoding_format?: "base64" | "float" | undefined;
        input: string | string[] | number[] | number[][];
    }, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        data: {
            index: number;
            embedding: number[];
        }[];
        model: string;
        usage: {
            prompt_tokens: number;
            completion_tokens?: number | undefined;
            total_tokens: number;
        };
        provider_name: string;
        provider_id: string;
    }>;
}

declare class export_default$4 extends APIModule {
    list: (options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        id: number;
        name: string;
        model_type?: "text2text" | "text2image" | "text2vector" | "autopilot" | undefined;
        model_provider?: "" | "openai" | "azure" | "azure-mistral" | "cohere" | "anthropic" | "cloudflare" | "deepinfra" | "lamini" | "octoai" | "replicate" | "together" | "fireworksai" | "mistralai" | "prem" | "anyscale" | "openrouter" | "perplexity" | "groq" | null | undefined;
        deprecated?: boolean | undefined;
    }[]>;
    retrieve: (id: string, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        id: number;
        name: string;
        model_type?: "text2text" | "text2image" | "text2vector" | "autopilot" | undefined;
        model_provider?: "" | "openai" | "azure" | "azure-mistral" | "cohere" | "anthropic" | "cloudflare" | "deepinfra" | "lamini" | "octoai" | "replicate" | "together" | "fireworksai" | "mistralai" | "prem" | "anyscale" | "openrouter" | "perplexity" | "groq" | null | undefined;
        deprecated?: boolean | undefined;
    }>;
}

declare class export_default$3 extends APIModule {
    create: (params: {
        id: number;
        name: string;
        description?: string | null | undefined;
        organization: string;
    }, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        id: number;
        name: string;
        description?: string | null | undefined;
        organization: string;
    }>;
}

declare class export_default$2 extends APIModule {
    create: (repository_id: string, params: any, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        repository_id: number;
        document_id: number;
        name: string;
        document_type: "pdf" | "docx" | "txt";
        status: "PENDING" | "UPLOADED" | "PARSING" | "CHUNKING" | "WAITING_FOR_CHUNKS_COMPLETION" | "PROCESSING" | "COMPLETED" | "FAILED";
        error: string | null;
        chunk_count: number;
    }>;
}

declare class export_default$1 extends APIModule {
    create: (params: {
        trace_id: string;
        feedback?: {
            positive?: boolean | undefined;
            messages?: {
                role?: "user" | "assistant" | "system" | undefined;
                content?: string | undefined;
            }[] | undefined;
        } | null | undefined;
    }, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        trace_id: string;
        feedback?: {
            positive?: boolean | undefined;
            messages?: {
                role?: "user" | "assistant" | "system" | undefined;
                content?: string | undefined;
            }[] | undefined;
        } | null | undefined;
    }>;
}

declare class export_default extends APIModule {
    list: (options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        trace_id: string;
        project_id: number;
        model_id: number;
        feedback: {
            positive: boolean;
            used_datapoint_messages: boolean;
            messages: {
                role?: "user" | "assistant" | "system" | undefined;
                content?: string | undefined;
            }[];
        } | null;
    }[]>;
    retrieve: (id: string, options?: axios.AxiosRequestConfig<any> | undefined) => Promise<{
        trace_id: string;
        project: {
            project_id: number;
            name: string;
            description?: string | null | undefined;
        };
        model: {
            id: number;
            name: string;
            model_type?: "text2text" | "text2image" | "text2vector" | "autopilot" | undefined;
            model_provider?: "" | "openai" | "azure" | "azure-mistral" | "cohere" | "anthropic" | "cloudflare" | "deepinfra" | "lamini" | "octoai" | "replicate" | "together" | "fireworksai" | "mistralai" | "prem" | "anyscale" | "openrouter" | "perplexity" | "groq" | null | undefined;
            deprecated?: boolean | undefined;
        };
        session_id?: string | null | undefined;
        messages?: {
            role?: "user" | "assistant" | "system" | undefined;
            content?: string | undefined;
        }[] | undefined;
        feedback: {
            positive: boolean;
            used_datapoint_messages: boolean;
            messages: {
                role?: "user" | "assistant" | "system" | undefined;
                content?: string | undefined;
            }[];
        } | null;
        document_chunks?: {
            document_id: number;
            content?: string | null | undefined;
        }[] | undefined;
    }>;
}

declare class Prem {
    config: PremBaseConfig & {
        baseUrl: string;
    };
    axios: Axios;
    chat: {
        completions: export_default$6;
    };
    embeddings: export_default$5;
    models: export_default$4;
    repositories: export_default$3;
    repository: {
        document: export_default$2;
    };
    feedbacks: export_default$1;
    traces: export_default;
    constructor(config: PremBaseConfig, axiosConfig?: CreateAxiosDefaults);
    call: <T>(request: AxiosRequestConfig) => Promise<T>;
}

export { APIResponseValidationError, AuthenticationError, CatchAllError, ChatCompletionStreamingCompletionData, ChatCompletionStreamingMessage, ConflictError, CreateChatCompletionRequest, CreateChatCompletionResponse, CreateDataPointRequest, CreateDataPointResponse, CreateEmbeddingRequest, CreateEmbeddingResponse, DocumentChunk, InternalServerError, ListDataPointResponse, ModelNotFoundError, NotFoundError, PermissionDeniedError, PremBaseConfig, ProviderAPIConnectionError, ProviderAPIStatusError, ProviderAPITimeoutError, ProviderInternalServerError, RetrieveDataPointResponse, UpdateDataPointRequest, UpdateDataPointResponse, ValidationError, Prem as default };
